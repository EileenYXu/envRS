---
title: "ABCDv5.1 Y2 sensitivity"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float:
      collapsed: true
editor_options:
  chunk_output_type: inline
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, results = "asis")
library(tidyverse)
library(RColorBrewer)
library(glmnet)
library(kableExtra)
set.seed(2024)

pal = paletteer::paletteer_d("colorblindr::OkabeIto")
```



# Sensitivity analyses using birthsex instead of gender

```{r}
train = readRDS("G://users/eileen/ABCD/ABCD_Environmental_Risk/ABCDv5.1/DATA/y2_traindat.rds") |> na.omit() |> filter(birthsex != "Intersex") |> droplevels()
test = readRDS("G://users/eileen/ABCD/ABCD_Environmental_Risk/ABCDv5.1/DATA/y2_testdat.rds") |> na.omit() |> filter(birthsex != "Intersex") |> droplevels()

trainIDs = train$src_subject_id
testIDs = test$src_subject_id
```


```{r}
## scale and centre numeric variables
numcols = names(train)[lapply(train, is.numeric)==T]
train = train |> mutate_at(numcols, ~ c(scale(.)) )
train$bullying_sex = train$bullying_victim * as.numeric(train$birthsex)

test = test |> mutate_at(numcols, ~ c(scale(.)) )
test$bullying_sex = test$bullying_victim * as.numeric(test$birthsex)

## remove columns that are not used for training data
preds = train |> select(-src_subject_id, -race_ethnicity, -cbcl_internalising, -cbcl_dsm5_depress, -gender) |> data.matrix()

## same for test data
test_preds = test |> select(-src_subject_id, -race_ethnicity, -cbcl_internalising, -cbcl_dsm5_depress, -gender) |> data.matrix()

## data for model
dep = train$cbcl_dsm5_depress
int = train$cbcl_internalising
```

Training sample `r nrow(train)`, test sample `r nrow(test)`.

# CBCL DSM5-oriented depression subscale {.tabset}

## Cross-validation

```{r}
## assign each observation to a fold so that I can cross-validate alpha
foldid = sample(1:10, size = length(dep), replace = TRUE)
alphas = seq(0, 1, by = 0.1)

## use for loop to cross-validate alphas
depfits = list()
for(i in 1:length(alphas)) {
  depfits[[paste0("alpha_", alphas[i])]] = cv.glmnet(x = preds, y = dep, foldid = foldid, type.measure = "deviance", alpha = alphas[i])
}

## use for loop to get fit statistics for each alpha value in training data
trainfits = list()
for (i in 1:length(depfits)) {
  trainfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(depfits[[i]], newx = preds, newy = dep, s = "lambda.min")
}

## same thing for test data
testfits = list()
for (i in 1:length(depfits)) {
  testfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(depfits[[i]], newx = test_preds, newy = test$cbcl_dsm5_depress, s = "lambda.min")
}

## extract MSE for training and test data and print this as a table
train_mse = sapply(trainfits, function(x) x$mse[["lambda.min"]])
test_mse = sapply(testfits, function(x) x$mse[["lambda.min"]])

data.frame(train_mse = train_mse, test_mse = test_mse) |>  kbl() |> kable_styling() |> scroll_box(height = "300px")

## plot MSE and log(Lambda) for each alpha value, with axis labels and legend
plot(log(depfits[[1]]$lambda), depfits[[1]]$cvm, col = pal[1], type = "l",
     xlab = "log(Lambda)", ylab = "Mean cross-validated error")
for (i in 2:length(depfits)) {
  lines(log(depfits[[i]]$lambda), depfits[[i]]$cvm, col = pal[i])
}
legend("topright", legend = paste0("alpha = ", alphas), col = pal, lty = 1)
```

Alpha value with the lowest MSE in training sample = `r rownames(data.frame(train_mse))[which.min(train_mse)]`. Coefficients from this model will be used.  

For comparison, alpha value with lowest MSE in test sample = `r rownames(data.frame(test_mse))[which.min(test_mse)]`

```{r results='hide'}
depcoef = coef(depfits[[which.min(train_mse)]], s = "lambda.min") |> as.matrix() |> as.data.frame() |> rownames_to_column(var = "preds")
names(depcoef)[2] = "depRS"
```

## Plot predictions

```{r}
depRS_train = data.frame(
  src_subject_id = trainIDs,
  cbcl_dep = dep,
  depRS.min = predict(depfits[[which.min(train_mse)]], newx = preds, s = "lambda.min"),
  data = rep("train", times= length(dep))
)

depRS_test = data.frame(
  src_subject_id = testIDs,
  cbcl_dep = test$cbcl_dsm5_depress,
  depRS.min = predict(depfits[[which.min(train_mse)]], newx = test_preds, s = "lambda.min"),
  data = rep("test", times = nrow(test))
)

depRS = rbind(depRS_train, depRS_test)
```

```{r}
ggplot(depRS, aes(y = cbcl_dep, x = lambda.min, colour = data)) +
  geom_smooth() +
  theme_bw() +
  labs(x = "predicted depRS", y = "CBCL depression (scaled and centred)")
```

# CBCL internalising subscale {.tabset}

## Cross-validation

```{r}
## assign each observation to a fold so that I can cross-validate alpha
foldid = sample(1:10, size = length(int), replace = TRUE)
alphas = seq(0, 1, by = 0.1)

## use for loop to cross-validate alphas
intfits = list()
for(i in 1:length(alphas)) {
  intfits[[paste0("alpha_", alphas[i])]] = cv.glmnet(x = preds, y = int, foldid = foldid, type.measure = "deviance", alpha = alphas[i])
}

## use for loop to get fit statistics for each alpha value in training data
trainfits = list()
for (i in 1:length(intfits)) {
  trainfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(intfits[[i]], newx = preds, newy = int, s = "lambda.min")
}

## same thing for test data
testfits = list()
for (i in 1:length(intfits)) {
  testfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(intfits[[i]], newx = test_preds, newy = test$cbcl_internalising, s = "lambda.min")
}

## extract MSE for training and test data and print this as a table
train_mse = sapply(trainfits, function(x) x$mse[["lambda.min"]])
test_mse = sapply(testfits, function(x) x$mse[["lambda.min"]])

data.frame(train_mse = train_mse, test_mse = test_mse) |>  kbl() |> kable_styling() |> scroll_box(height = "300px")

## plot MSE and log(Lambda) for each alpha value, with axis labels and legend
plot(log(intfits[[1]]$lambda), intfits[[1]]$cvm, col = pal[1], type = "l",
     xlab = "log(Lambda)", ylab = "Mean cross-validated error")
for (i in 2:length(intfits)) {
  lines(log(intfits[[i]]$lambda), intfits[[i]]$cvm, col = pal[i])
}
legend("topright", legend = paste0("alpha = ", alphas), col = pal, lty = 1)
```

Alpha value with the lowest MSE in training sample = `r rownames(data.frame(train_mse))[which.min(train_mse)]`. Coefficients from this model will be used.  

For comparison, alpha value with lowest MSE in test sample = `r rownames(data.frame(test_mse))[which.min(test_mse)]`

```{r results='hide'}
intcoef = coef(intfits[[which.min(train_mse)]], s = "lambda.min") |> as.matrix() |> as.data.frame()|> rownames_to_column(var = "preds")
names(intcoef)[2] = "intRS"
```

## Plot predictions

```{r}
intRS_train = data.frame(
  src_subject_id = trainIDs,
  cbcl_int = int,
  intRS.min = predict(intfits[[which.min(train_mse)]], newx = preds, s = "lambda.min"),
  data = rep("train", times= length(int))
)

intRS_test = data.frame(
  src_subject_id = testIDs,
  cbcl_int = test$cbcl_internalising,
  intRS.min = predict(intfits[[which.min(train_mse)]], newx = test_preds, s = "lambda.min"),
  data = rep("test", times = nrow(test))
)

intRS = rbind(intRS_train, intRS_test)
```

```{r}
ggplot(intRS, aes(y = cbcl_int, x = lambda.min, colour = data)) +
  geom_smooth() +
  theme_bw() +
  labs(x = "predicted intRS", y = "CBCL internalising (scaled and centred)")
```

# Non-zero coefficients {.tabset}

```{r}
y2_coefs = merge(depcoef, intcoef)

write.csv(y2_coefs, "C://users/eilee/Desktop/PhD/Year 3/Drafts/envRS_paper/y2_coefs_sens.csv")
```

## CBCL depression

```{r}
depcoef[order(depcoef$depRS, decreasing = T),] |> filter(abs(depRS)!=0) |> kbl(digits=3) |> kable_styling() |> scroll_box(height = "300px")
```

## CBCL internalising

```{r}
intcoef[order(intcoef$intRS, decreasing = T),] |> filter(abs(intRS)!=0) |> kbl(digits=3) |> kable_styling() |> scroll_box(height = "300px")
```
