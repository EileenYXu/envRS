---
title: "ABCDv5.1 Baseline Sensitivity"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float:
      collapsed: true
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = "asis")
library(tidyverse)
library(RColorBrewer)
library(glmnet)
library(kableExtra)
set.seed(2024)

pal = paletteer::paletteer_d("colorblindr::OkabeIto")
```

# Sensitivity analyses using birthsex instead of gender

```{r}
## read in data, exclude individuals identifying as Intersex (too few for meaningful effect from interaction I think)
dat = readRDS("G://users/eileen/ABCD/ABCD_Environmental_Risk/ABCDv5.1/DATA/dat_baseline.rds") |> filter(birthsex != "Intersex") |> droplevels()

## split into train and test samples 
index = caret::createDataPartition(dat$cbcl_dsm5_depress, p = 0.2,
                                   list = F, times = 1)
train = dat[-index,]
test = dat[index,]
```


```{r}
sumdat = list("whole"=rbind(train, test), "train" = train, "test" = test)

trainIDs = train$src_subject_id
testIDs = test$src_subject_id

## scale and centre numeric variables
numcols = names(train)[lapply(train, is.numeric)==T]
train = train |> mutate_at(numcols, ~ c(scale(.)) )
test = test |> mutate_at(numcols, ~ c(scale(.)) )

## remove columns that are not used for training data
preds = train |> select(-src_subject_id, -race_ethnicity, -cbcl_internalising, -cbcl_dsm5_depress, -birthsex, -gender_id) |> data.matrix()

## same for test data
test_preds = test |> select(-src_subject_id, -race_ethnicity, -cbcl_internalising, -cbcl_dsm5_depress, -birthsex, -gender_id) |> data.matrix()

## data for model
dep = train$cbcl_dsm5_depress
int = train$cbcl_internalising
```

80-20 training-test split. Training sample N=`r nrow(train)`, test sample N=`r nrow(test)`. Although glmnet standardises data by default, coefficients are returned on their original scales. To get standardised coefficients for numeric variables (easier to apply to other data), I'm scaling the raw data before inputting to glmnet. Each elastic net model is run with 10-fold cross-validation and lambda/alpha values are selected based on minimising the model mean squared error (MSE).


`r ncol(preds)` predictors were included. No interactions with gender were included, as the 3 significant interactions (cannabis, social media, bullying) aren't present in baseline data.

# CBCL DSM5-oriented depression subscale {.tabset}

## Cross-validation

```{r}
## assign each observation to a fold so that I can cross-validate alpha
foldid = sample(1:10, size = length(dep), replace = TRUE)
alphas = seq(0, 1, by = 0.1)

## use for loop to cross-validate alphas
depfits = list()
for(i in 1:length(alphas)) {
  depfits[[paste0("alpha_", alphas[i])]] = cv.glmnet(x = preds, y = dep, foldid = foldid, type.measure = "deviance", alpha = alphas[i])
}

## use for loop to get fit statistics for each alpha value in training data
trainfits = list()
for (i in 1:length(depfits)) {
  trainfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(depfits[[i]], newx = preds, newy = dep, s = "lambda.min")
}

## same thing for test data
testfits = list()
for (i in 1:length(depfits)) {
  testfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(depfits[[i]], newx = test_preds, newy = test$cbcl_dsm5_depress, s = "lambda.min")
}

## extract MSE for training and test data and print this as a table
train_mse = sapply(trainfits, function(x) x$mse[["lambda.min"]])
test_mse = sapply(testfits, function(x) x$mse[["lambda.min"]])

data.frame(train_mse = train_mse, test_mse = test_mse) |>  kbl() |> kable_styling() |> scroll_box(height = "300px")

## plot MSE and log(Lambda) for each alpha value, with axis labels and legend
plot(log(depfits[[1]]$lambda), depfits[[1]]$cvm, col = pal[1], type = "l",
     xlab = "log(Lambda)", ylab = "Mean cross-validated error")
for (i in 2:length(depfits)) {
  lines(log(depfits[[i]]$lambda), depfits[[i]]$cvm, col = pal[i])
}
legend("topright", legend = paste0("alpha = ", alphas), col = pal, lty = 1)
```

Alpha value with the lowest MSE in training sample = `r rownames(data.frame(train_mse))[which.min(train_mse)]`. Coefficients from this model will be used.  

For comparison, alpha value with lowest MSE in test sample = `r rownames(data.frame(test_mse))[which.min(test_mse)]`

```{r results='hide'}
depcoef = coef(depfits[[which.min(train_mse)]], s = "lambda.min") |> as.matrix() |> as.data.frame() |> rownames_to_column(var = "preds")
names(depcoef)[2] = "depRS"
```

## Plot predictions

```{r}
depRS_train = data.frame(
  src_subject_id = trainIDs,
  cbcl_dep = dep,
  depRS.min = predict(depfits[[which.min(train_mse)]], newx = preds, s = "lambda.min"),
  data = rep("train", times= length(dep))
)

depRS_test = data.frame(
  src_subject_id = testIDs,
  cbcl_dep = test$cbcl_dsm5_depress,
  depRS.min = predict(depfits[[which.min(train_mse)]], newx = test_preds, s = "lambda.min"),
  data = rep("test", times = nrow(test))
)

depRS = rbind(depRS_train, depRS_test)
```

```{r}
ggplot(depRS, aes(y = cbcl_dep, x = lambda.min, colour = data)) +
  geom_smooth() +
  theme_bw() +
  labs(x = "predicted depRS", y = "CBCL depression (scaled and centred)")
```

## R-square

```{r}
testlm = lm(cbcl_dep ~ lambda.min, depRS_test)
trainlm = lm(cbcl_dep ~ lambda.min, depRS_train)
bothlm = lm(cbcl_dep ~ lambda.min, depRS)

sjPlot::tab_model(list(testlm, trainlm, bothlm), dv.labels = c("test", "train", "whole"))
```

# CBCL internalising subscale {.tabset}

## Cross-validation

```{r}
## assign each observation to a fold so that I can cross-validate alpha
foldid = sample(1:10, size = length(int), replace = TRUE)
alphas = seq(0, 1, by = 0.1)

## use for loop to cross-validate alphas
intfits = list()
for(i in 1:length(alphas)) {
  intfits[[paste0("alpha_", alphas[i])]] = cv.glmnet(x = preds, y = int, foldid = foldid, type.measure = "deviance", alpha = alphas[i])
}

## use for loop to get fit statistics for each alpha value in training data
trainfits = list()
for (i in 1:length(intfits)) {
  trainfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(intfits[[i]], newx = preds, newy = int, s = "lambda.min")
}

## same thing for test data
testfits = list()
for (i in 1:length(intfits)) {
  testfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(intfits[[i]], newx = test_preds, newy = test$cbcl_internalising, s = "lambda.min")
}

## extract MSE for training and test data and print this as a table
train_mse = sapply(trainfits, function(x) x$mse[["lambda.min"]])
test_mse = sapply(testfits, function(x) x$mse[["lambda.min"]])

data.frame(train_mse = train_mse, test_mse = test_mse) |>  kbl() |> kable_styling() |> scroll_box(height = "300px")

## plot MSE and log(Lambda) for each alpha value, with axis labels and legend
plot(log(intfits[[1]]$lambda), intfits[[1]]$cvm, col = pal[1], type = "l",
     xlab = "log(Lambda)", ylab = "Mean cross-validated error")
for (i in 2:length(intfits)) {
  lines(log(intfits[[i]]$lambda), intfits[[i]]$cvm, col = pal[i])
}
legend("topright", legend = paste0("alpha = ", alphas), col = pal, lty = 1)
```

Alpha value with the lowest MSE in training sample = `r rownames(data.frame(train_mse))[which.min(train_mse)]`. Coefficients from this model will be used.  

```{r results='hide'}
intcoef = coef(intfits[[which.min(train_mse)]], s = "lambda.min") |> as.matrix() |> as.data.frame() |> rownames_to_column(var = "preds")
names(intcoef)[2] = "intRS"
```

## Plot predictions

```{r}
intRS_train = data.frame(
  src_subject_id = trainIDs,
  cbcl_int = int,
  intRS.min = predict(intfits[[which.min(train_mse)]], newx = preds, s = "lambda.min"),
  data = rep("train", times= length(int))
)

intRS_test = data.frame(
  src_subject_id = testIDs,
  cbcl_int = test$cbcl_internalising,
  intRS.min = predict(intfits[[which.min(train_mse)]], newx = test_preds, s = "lambda.min"),
  data = rep("test", times = nrow(test))
)

intRS = rbind(intRS_train, intRS_test)
```

```{r}
ggplot(intRS, aes(y = cbcl_int, x = lambda.min, colour = data)) +
  geom_smooth() +
  theme_bw() +
  labs(x = "predicted intRS", y = "CBCL internalising (scaled and centred)")
```

# Non-zero coefficients {.tabset}

```{r}
base_coefs = merge(depcoef, intcoef)

# make row names the first column
base_coefs = base_coefs

write.csv(base_coefs, "C://users/eilee/Desktop/PhD/Year 3/Drafts/envRS_paper/baseline_coefs_sens.csv")
```

## CBCL depression

```{r}
depcoef[order(depcoef$depRS, decreasing = T),] |> filter(abs(depRS)!=0) |> kbl(digits=3) |> kable_styling() |> scroll_box(height = "300px")
```

## CBCL internalising

```{r}
intcoef[order(intcoef$intRS, decreasing = T),] |> filter(abs(intRS)!=0) |> kbl(digits=3) |> kable_styling() |> scroll_box(height = "300px")
```


