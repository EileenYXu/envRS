---
title: "ABCDv5.1 Longitudinal depRS"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, message = F, results = "asis", warning = F)
library(tidyverse)
library(kableExtra)
options(knitr.kable.NA = '')
library(RColorBrewer)
library(glmnet)
pal = paletteer::paletteer_d("colorblindr::OkabeIto")
set.seed(2024)
```

# Elastic net linear regression using 'glmnet'

Model predicting year 2 CBCL using environmental factors measured at baseline.

## Data prep

```{r}
dat = readRDS("G://users/eileen/ABCD/ABCD_Environmental_Risk/ABCDv5.1/DATA/predictors_unrelatedIDs.rds")

y2CBCL = dat |> filter(eventname=="2_year_follow_up_y_arm_1") |> select(src_subject_id, interview_age, cbcl_scr_dsm5_depress_r, cbcl_scr_syn_internal_r) |> drop_na(cbcl_scr_dsm5_depress_r, cbcl_scr_dsm5_depress_r) |> 
  mutate(y2_age = interview_age/12) |> select(-interview_age)

baseline = dat |> select(-cbcl_scr_syn_internal_r, -cbcl_scr_dsm5_depress_r) |> filter(eventname=="baseline_year_1_arm_1") |> select_if(~ !all(is.na(.))) |> droplevels()

#merge in y2 CBCL
alldat = merge(baseline, y2CBCL, by = "src_subject_id")
```

N = `r nrow(y2CBCL)` individuals with CBCL data at year 2

```{r}
## remove baseline features with >20% missing data
dat.mod = alldat |> select_if(~ sum(is.na(.))<0.2*nrow(alldat))

dat.mod = dat.mod |> select(-c(eventname, illicit, site_id_l, rel_family_id, rel_birth_id, interview_date, visit_type, race_ethnicity, y2_age))

names(dat.mod) = c("src_subject_id", "tobacco_puff", "weightcontrol_ksads",
                    "witness_comm_violence", "death_loved_one",
                    "witness_dv", "s_abuse", "p_abuse", "emot_abuse",
                    "serious_accident", "sleep_hrs", "bmi", "needed_food",
                    "income", "parent_ed", "birthsex", "gender_id", "gender",
                    "area_depriv", "comm_safety", "days_active",
                    "fam_conflict", "p_monitoring", "p_acceptance",
                    "p_depression", "agemths",
                    "y2_cbcl_dep", "y2_cbcl_int")

# remove missing data
dat.mod = dat.mod |> na.omit() |> droplevels()
```

N = `r nrow(dat.mod)` individuals with complete CBCL and predictor data. \
N = `r nrow(dat.mod |> filter(gender == "GNC"))` individuals assigned intersex at birth are then excluded before splitting the training and test samples.

```{r}
## remove GNC individuals, then split training and test samples ##
dat = dat.mod |> filter(birthsex != "Intersex") |> droplevels()

## split into train and test samples ##
index = caret::createDataPartition(dat$y2_cbcl_dep[which(is.na(dat$y2_cbcl_dep)==F)], 
                            p = 0.2, list = F, times = 1)
train = dat[-index,]
test = dat[index,]
```

Total N = `r nrow(dat)` split into training 80/% (N = `r nrow(train)`) and test 20/% (N = `r nrow(test)`) samples. \

# CBCL DSM5-oriented depression subscale {.tabset}

```{r}
trainIDs = train$src_subject_id
testIDs = test$src_subject_id

#for summary stats
sumdat = list("whole"=alldat[which(alldat$src_subject_id %in% c(trainIDs, testIDs)),],
              "train" = alldat[which(alldat$src_subject_id %in% c(trainIDs)),], 
              "test" = alldat[which(alldat$src_subject_id %in% c(testIDs)),])

## scale and centre numeric variables
numcols = names(train)[lapply(train, is.numeric)==T]
train = train |> mutate_at(numcols, ~ c(scale(.)) )
test = test |> mutate_at(numcols, ~ c(scale(.)) )

## remove columns that are not used for training data
preds = train |> select(-src_subject_id, -gender, -gender_id, -y2_cbcl_dep, -y2_cbcl_int) |> data.matrix()

## same for test data
test_preds = test |> select(-src_subject_id, -gender, -gender_id, -y2_cbcl_dep, -y2_cbcl_int) |> data.matrix()

## data for model
dep = train$y2_cbcl_dep
int = train$y2_cbcl_int
```

`r ncol(preds)` predictors were included. \

Although glmnet standardises data by default, coefficients are returned on their original scales. To get standardised coefficients for applying to other data, I'm scaling the raw data before inputting to glmnet. Each elastic net model is run with 10-fold cross-validation and lambda/alpha values are selected based on minimising the model mean squared error (MSE).

## Cross-validation

```{r}
## assign each observation to a fold so that I can cross-validate alpha
foldid = sample(1:10, size = length(dep), replace = TRUE)
alphas = seq(0, 1, by = 0.1)

## use for loop to cross-validate alphas
depfits = list()
for(i in 1:length(alphas)) {
  depfits[[paste0("alpha_", alphas[i])]] = cv.glmnet(x = preds, y = dep, foldid = foldid, type.measure = "deviance", alpha = alphas[i])
}

## use for loop to get fit statistics for each alpha value in training data
trainfits = list()
for (i in 1:length(depfits)) {
  trainfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(depfits[[i]], newx = preds, newy = dep, s = "lambda.min")
}

## same thing for test data
testfits = list()
for (i in 1:length(depfits)) {
  testfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(depfits[[i]], newx = test_preds, newy = test$y2_cbcl_dep, s = "lambda.min")
}

## extract MSE for training and test data and print this as a table
train_mse = sapply(trainfits, function(x) x$mse[["lambda.min"]])
test_mse = sapply(testfits, function(x) x$mse[["lambda.min"]])

data.frame(train_mse = train_mse, test_mse = test_mse) |>  kbl() |> kable_styling() |> scroll_box(height = "300px")
```

LASSO: $\alpha$ = 1; Ridge: $\alpha$ = 0 

Alpha value with the lowest MSE in training sample = `r rownames(data.frame(train_mse))[which.min(train_mse)]`. Coefficients from this model will be used.  

```{r results='hide'}
depcoef = coef(depfits[[which.min(train_mse)]], s = "lambda.min") |> as.matrix() |> as.data.frame() |> rownames_to_column(var = "preds")
names(depcoef)[2] = "depRS"
```

## Plot alphas

```{r}
## plot MSE and log(Lambda) for each alpha value, with axis labels and legend
plot(log(depfits[[1]]$lambda), depfits[[1]]$cvm, col = pal[1], type = "l",
     xlab = "log(Lambda)", ylab = "Mean cross-validated error")
for (i in 2:length(depfits)) {
  lines(log(depfits[[i]]$lambda), depfits[[i]]$cvm, col = pal[i])
}
legend("topright", legend = paste0("alpha = ", alphas), col = pal, lty = 1)
```


## Plot predictions

```{r}
depRS_train = data.frame(
  src_subject_id = trainIDs,
  cbcl_dep = dep,
  depRS.min = predict(depfits[[which.min(train_mse)]], newx = preds, s = "lambda.min"),
  data = rep("train", times= length(dep))
)

depRS_test = data.frame(
  src_subject_id = testIDs,
  cbcl_dep = test$y2_cbcl_dep,
  depRS.min = predict(depfits[[which.min(train_mse)]], newx = test_preds, s = "lambda.min"),
  data = rep("test", times = nrow(test))
)

depRS = rbind(depRS_train, depRS_test)
```

```{r}
ggplot(depRS, aes(y = cbcl_dep, x = lambda.min, colour = data)) +
  geom_smooth() +
  theme_bw() +
  labs(x = "predicted depRS", y = "CBCL depression (scaled and centred)")
```

## R-square

```{r}
testlm = lm(cbcl_dep ~ lambda.min, depRS_test)
trainlm = lm(cbcl_dep ~ lambda.min, depRS_train)
bothlm = lm(cbcl_dep ~ lambda.min, depRS)

sjPlot::tab_model(list(testlm, trainlm, bothlm), dv.labels = c("test", "train", "whole"))
```

# CBCL internalising subscale {.tabset}

## Cross-validation

```{r}
## assign each observation to a fold so that I can cross-validate alpha
foldid = sample(1:10, size = length(int), replace = TRUE)
alphas = seq(0, 1, by = 0.1)

## use for loop to cross-validate alphas
intfits = list()
for(i in 1:length(alphas)) {
  intfits[[paste0("alpha_", alphas[i])]] = cv.glmnet(x = preds, y = int, foldid = foldid, type.measure = "deviance", alpha = alphas[i])
}

## use for loop to get fit statistics for each alpha value in training data
trainfits = list()
for (i in 1:length(intfits)) {
  trainfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(intfits[[i]], newx = preds, newy = int, s = "lambda.min")
}

## same thing for test data
testfits = list()
for (i in 1:length(intfits)) {
  testfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(intfits[[i]], newx = test_preds, newy = test$y2_cbcl_int, s = "lambda.min")
}

## extract MSE for training and test data and print this as a table
train_mse = sapply(trainfits, function(x) x$mse[["lambda.min"]])
test_mse = sapply(testfits, function(x) x$mse[["lambda.min"]])

data.frame(train_mse = train_mse, test_mse = test_mse) |>  kbl() |> kable_styling() |> scroll_box(height = "300px")
```

Alpha ranges from 0 (ridge regression) to 1 (LASSO)

Alpha value with the lowest MSE in training sample = `r rownames(data.frame(train_mse))[which.min(train_mse)]`. Coefficients from this model will be used.  

```{r results='hide'}
intcoef = coef(intfits[[which.min(train_mse)]], s = "lambda.min") |> as.matrix() |> as.data.frame() |> rownames_to_column(var = "preds")
names(intcoef)[2] = "intRS"
```

## Plot alphas

```{r}
## plot MSE and log(Lambda) for each alpha value, with axis labels and legend
plot(log(intfits[[1]]$lambda), intfits[[1]]$cvm, col = pal[1], type = "l",
     xlab = "log(Lambda)", ylab = "Mean cross-validated error")
for (i in 2:length(intfits)) {
  lines(log(intfits[[i]]$lambda), intfits[[i]]$cvm, col = pal[i])
}
legend("topright", legend = paste0("alpha = ", alphas), col = pal, lty = 1)
```


## Plot predictions

```{r}
intRS_train = data.frame(
  src_subject_id = trainIDs,
  cbcl_int = int,
  intRS.min = predict(intfits[[which.min(train_mse)]], newx = preds, s = "lambda.min"),
  data = rep("train", times= length(int))
)

intRS_test = data.frame(
  src_subject_id = testIDs,
  cbcl_int = test$y2_cbcl_int,
  intRS.min = predict(intfits[[which.min(train_mse)]], newx = test_preds, s = "lambda.min"),
  data = rep("test", times = nrow(test))
)

intRS = rbind(intRS_train, intRS_test)
```

```{r}
ggplot(intRS, aes(y = cbcl_int, x = lambda.min, colour = data)) +
  geom_smooth() +
  theme_bw() +
  labs(x = "predicted intRS", y = "CBCL internalising (scaled and centred)")
```

## R-square

```{r}
testlm = lm(cbcl_int ~ lambda.min, intRS_test)
trainlm = lm(cbcl_int ~ lambda.min, intRS_train)
bothlm = lm(cbcl_int ~ lambda.min, intRS)

sjPlot::tab_model(list(testlm, trainlm, bothlm), dv.labels = c("test", "train", "whole"))
```

# Coefficients {.tabset}

## Plot

```{r}
base_coefs = merge(depcoef, intcoef)

# make row names the first column
base_coefs = base_coefs

write.csv(base_coefs, "C://users/eilee/Desktop/PhD/Year 3/Drafts/envRS_paper/long_coefs_sens.csv")
```


## CBCL depression

```{r}
## nonzero coefficients for depression only
depcoef[order(depcoef$depRS, decreasing = T),] |> filter(abs(depRS)!=0) |> kbl(digits=3) |> kable_styling() |> scroll_box(height = "300px")
```

## CBCL internalising

```{r}
intcoef[order(intcoef$intRS, decreasing = T),] |> filter(abs(intRS)!=0) |> kbl(digits=3) |> kable_styling() |> scroll_box(height = "300px")
```

# Descriptives {.tabset}

```{r}
#get age in years
sumdat = lapply(sumdat, function(x) {mutate(x, age_yrs = interview_age/12)})
```

## Whole sample

```{r results='asis'}
summarytools::dfSummary(sumdat$whole[,-1],plain.ascii = F, style = "grid", valid.col = F, graph.col = T, varnumbers = F, tmp.img.dir = "/tmp", max.tbl.height = 300) |> print(method = "render")
```

## Training sample

```{r results='asis'}
summarytools::dfSummary(sumdat$train[,-1],plain.ascii = F, style = "grid", valid.col = F, graph.col = T, varnumbers = F, tmp.img.dir = "/tmp", max.tbl.height = 300) |> print(method = "render")
```

## Test sample

```{r results='asis'}
summarytools::dfSummary(sumdat$test[,-1],plain.ascii = F, style = "grid", valid.col = F, graph.col = T, varnumbers = F, tmp.img.dir = "/tmp", max.tbl.height = 300) |> print(method = "render")
```
