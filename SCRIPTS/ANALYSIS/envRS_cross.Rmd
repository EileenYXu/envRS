---
title: "ABCDv5.1 Baseline Scores"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float:
      collapsed: true
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = "asis")
library(tidyverse)
library(RColorBrewer)
library(glmnet)
library(kableExtra)
pal = paletteer::paletteer_d("colorblindr::OkabeIto")
set.seed(2024)

source("summarystats.R")
```

# Elastic net linear regression using 'glmnet'

Predictors identified from quantitative reviews of adolescent MDD risk factors in baseline ABCD data are included in the model, along with interactions with gender (parent-reported child gender identity). I will also fit a model with interactions with birth sex too as a sensitivity analysis. 

```{r}
## read in data, exclude individuals identifying as GNC (too few for meaningful effect from interaction I think)
dat = readRDS("/exports/igmm/datastore/GenScotDepressionusers/eileen/ABCD/ABCD_Environmental_Risk/ABCDv5.1/DATA/dat_baseline.rds") |> filter(gender != "GNC") |> droplevels()

## split into train and test samples 
index = caret::createDataPartition(dat$cbcl_dsm5_depress, p = 0.2,
                                   list = F, times = 1)
train = dat[-index,]
test = dat[index,]
```


```{r}
sumdat = list("whole"=rbind(train, test), "train" = train, "test" = test)

trainIDs = train$src_subject_id
testIDs = test$src_subject_id

## scale and centre numeric variables
numcols = names(train)[lapply(train, is.numeric)==T]
train = train |> mutate_at(numcols, ~ c(scale(.)) )
test = test |> mutate_at(numcols, ~ c(scale(.)) )

## remove columns that are not used for training data
preds = train |> select(-src_subject_id, -race_ethnicity, -cbcl_internalising, -cbcl_dsm5_depress, -birthsex, -gender_id) |> data.matrix()

## same for test data
test_preds = test |> select(-src_subject_id, -race_ethnicity, -cbcl_internalising, -cbcl_dsm5_depress, -birthsex, -gender_id) |> data.matrix()

## data for model
dep = train$cbcl_dsm5_depress
int = train$cbcl_internalising
```

80-20 training-test split. Training sample N=`r nrow(train)`, test sample N=`r nrow(test)`. Although glmnet standardises data by default, coefficients are returned on their original scales. To get standardised coefficients for numeric variables (easier to apply to other data), I'm scaling the raw data before inputting to glmnet. Each elastic net model is run with 10-fold cross-validation and lambda/alpha values are selected based on minimising the model mean squared error (MSE).


`r ncol(preds)` predictors were included. No interactions with gender were included, as the 3 significant interactions (cannabis, social media, bullying) aren't present in baseline data.

# CBCL DSM5-oriented depression subscale {.tabset}

## Cross-validation

```{r}
## assign each observation to a fold so that I can cross-validate alpha
foldid = sample(1:10, size = length(dep), replace = TRUE)
alphas = seq(0, 1, by = 0.1)

## use for loop to cross-validate alphas
depfits = list()
for(i in 1:length(alphas)) {
  depfits[[paste0("alpha_", alphas[i])]] = cv.glmnet(x = preds, y = dep, foldid = foldid, type.measure = "deviance", alpha = alphas[i])
}

## use for loop to get fit statistics for each alpha value in training data
trainfits = list()
for (i in 1:length(depfits)) {
  trainfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(depfits[[i]], newx = preds, newy = dep, s = "lambda.min")
}

## same thing for test data
testfits = list()
for (i in 1:length(depfits)) {
  testfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(depfits[[i]], newx = test_preds, newy = test$cbcl_dsm5_depress, s = "lambda.min")
}

## extract MSE for training and test data and print this as a table
train_mse = sapply(trainfits, function(x) x$mse[["lambda.min"]])
test_mse = sapply(testfits, function(x) x$mse[["lambda.min"]])

data.frame(train_mse = train_mse, test_mse = test_mse) |>  kbl() |> kable_styling() |> scroll_box(height = "300px")
```

LASSO: $\alpha = 1$, Ridge: $\alpha = 0$.

Alpha value with the lowest MSE in training sample = `r rownames(data.frame(train_mse))[which.min(train_mse)]`. Coefficients from this model will be used.  

```{r results='hide'}
depcoef = coef(depfits[[which.min(train_mse)]], s = "lambda.min") |> as.matrix() |> as.data.frame() |> rownames_to_column(var = "preds")
names(depcoef)[2] = "depRS"
```

## Plot alpha

```{r}
## plot MSE and log(Lambda) for each alpha value, with axis labels and legend
plot(log(depfits[[1]]$lambda), depfits[[1]]$cvm, col = pal[1], type = "l",
     xlab = "log(Lambda)", ylab = "Mean cross-validated error")
for (i in 2:length(depfits)) {
  lines(log(depfits[[i]]$lambda), depfits[[i]]$cvm, col = pal[i])
}
legend("topright", legend = paste0("alpha = ", alphas), col = pal, lty = 1)
```


## Plot predictions

```{r}
depRS_train = data.frame(
  src_subject_id = trainIDs,
  cbcl_dep = dep,
  depRS.min = predict(depfits[[which.min(train_mse)]], newx = preds, s = "lambda.min"),
  data = rep("train", times= length(dep))
)

depRS_test = data.frame(
  src_subject_id = testIDs,
  cbcl_dep = test$cbcl_dsm5_depress,
  depRS.min = predict(depfits[[which.min(train_mse)]], newx = test_preds, s = "lambda.min"),
  data = rep("test", times = nrow(test))
)

depRS = rbind(depRS_train, depRS_test)

saveRDS(depRS, "/exports/igmm/datastore/GenScotDepressionusers/eileen/ABCD/ABCD_Environmental_Risk/ABCDv5.1/DATA/depRS_baseline.rds")
```

```{r}
ggplot(depRS, aes(y = cbcl_dep, x = lambda.min, colour = data)) +
  geom_smooth() +
  theme_bw() +
  labs(x = "predicted depRS", y = "CBCL depression (scaled and centred)")
```

## R-square

```{r}
testlm = lm(cbcl_dep ~ lambda.min, depRS_test)
trainlm = lm(cbcl_dep ~ lambda.min, depRS_train)
bothlm = lm(cbcl_dep ~ lambda.min, depRS)

sjPlot::tab_model(list(testlm, trainlm, bothlm), dv.labels = c("test", "train", "whole"))
```

# CBCL internalising subscale {.tabset}

## Cross-validation

```{r}
## assign each observation to a fold so that I can cross-validate alpha
foldid = sample(1:10, size = length(int), replace = TRUE)
alphas = seq(0, 1, by = 0.1)

## use for loop to cross-validate alphas
intfits = list()
for(i in 1:length(alphas)) {
  intfits[[paste0("alpha_", alphas[i])]] = cv.glmnet(x = preds, y = int, foldid = foldid, type.measure = "deviance", alpha = alphas[i])
}

## use for loop to get fit statistics for each alpha value in training data
trainfits = list()
for (i in 1:length(intfits)) {
  trainfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(intfits[[i]], newx = preds, newy = int, s = "lambda.min")
}

## same thing for test data
testfits = list()
for (i in 1:length(intfits)) {
  testfits[[paste0("alpha_", alphas[i])]] = assess.glmnet(intfits[[i]], newx = test_preds, newy = test$cbcl_internalising, s = "lambda.min")
}

## extract MSE for training and test data and print this as a table
train_mse = sapply(trainfits, function(x) x$mse[["lambda.min"]])
test_mse = sapply(testfits, function(x) x$mse[["lambda.min"]])

data.frame(train_mse = train_mse, test_mse = test_mse) |>  kbl() |> kable_styling() |> scroll_box(height = "300px")

```

LASSO: $\alpha = 1$, Ridge: $\alpha = 0$.

Alpha value with the lowest MSE in training sample = `r rownames(data.frame(train_mse))[which.min(train_mse)]`. Coefficients from this model will be used.  


```{r results='hide'}
intcoef = coef(intfits[[which.min(train_mse)]], s = "lambda.min") |> as.matrix() |> as.data.frame() |> rownames_to_column(var = "preds")
names(intcoef)[2] = "intRS"
```

## Plot alpha

```{r}
## plot MSE and log(Lambda) for each alpha value, with axis labels and legend
plot(log(intfits[[1]]$lambda), intfits[[1]]$cvm, col = pal[1], type = "l",
     xlab = "log(Lambda)", ylab = "Mean cross-validated error")
for (i in 2:length(intfits)) {
  lines(log(intfits[[i]]$lambda), intfits[[i]]$cvm, col = pal[i])
}
legend("topright", legend = paste0("alpha = ", alphas), col = pal, lty = 1)
```


## Plot predictions

```{r}
intRS_train = data.frame(
  src_subject_id = trainIDs,
  cbcl_int = int,
  intRS.min = predict(intfits[[which.min(train_mse)]], newx = preds, s = "lambda.min"),
  data = rep("train", times= length(int))
)

intRS_test = data.frame(
  src_subject_id = testIDs,
  cbcl_int = test$cbcl_internalising,
  intRS.min = predict(intfits[[which.min(train_mse)]], newx = test_preds, s = "lambda.min"),
  data = rep("test", times = nrow(test))
)

intRS = rbind(intRS_train, intRS_test)

saveRDS(intRS, "/exports/igmm/datastore/GenScotDepressionusers/eileen/ABCD/ABCD_Environmental_Risk/ABCDv5.1/DATA/intRS_baseline.rds")
```

```{r}
ggplot(intRS, aes(y = cbcl_int, x = lambda.min, colour = data)) +
  geom_smooth() +
  theme_bw() +
  labs(x = "predicted intRS", y = "CBCL internalising (scaled and centred)")
```

## R-square

```{r}
testlm = lm(cbcl_int ~ lambda.min, intRS_test)
trainlm = lm(cbcl_int ~ lambda.min, intRS_train)
bothlm = lm(cbcl_int ~ lambda.min, intRS)

sjPlot::tab_model(list(testlm, trainlm, bothlm), dv.labels = c("test", "train", "whole"))
```

# Coefficients {.tabset}

## Plot

```{r}
base_coefs = merge(depcoef, intcoef)

# make row names the first column
base_coefs = base_coefs
```

```{r fig.height=20, fig.width=10, fig.cap="Nonzero"}
## plot coefficients for depRS and intRS
coef_long = pivot_longer(base_coefs[-1,], cols = c("depRS", "intRS"), names_to = "out", values_to = "beta.min")

base_coefs_sig = coef_long |> filter(abs(round(beta.min, 3))!=0)

## pivot this back to wide
coefs_sig = base_coefs_sig |> pivot_wider(names_from = out, values_from = beta.min)

coefs = list(base_coefs, coefs_sig)

openxlsx::write.xlsx(coefs, "/exports/igmm/datastore/GenScotDepressionusers/eileen/ABCD/ABCD_Environmental_Risk/ABCDv5.1/envRS_coefs_cross.xlsx")
```

```{r fig.height=15, fig.width=10}
ggplot(base_coefs_sig, aes(x = out, y = preds, fill = beta.min)) +
  geom_tile(color = "white", lwd = 1, linetype = 1) +
  geom_text(aes(label = round(beta.min, 3)), color = "black", size = 12, size.unit = "pt") +
  scale_fill_gradient2(name = "Standardised \n Beta", low = "#2166AC", high = "#B2182B") +
  xlab(NULL) + ylab(NULL) +
  theme(panel.grid = element_blank(), panel.background = element_blank(), 
        axis.text = element_text(size = 12), legend.title = element_text(size=12))

#ggsave(filename = "C:/Users/eilee/Desktop/PhD/Year 3/envRS/SCRIPTS/OUTPUT/baseline_coefs_sig.svg", height = 200, width = 200, units = "mm")
```

## CBCL depression

```{r}
## nonzero coefficients for depression only
depcoef[order(depcoef$depRS, decreasing = T),] |> filter(abs(depRS)!=0) |> kbl(digits=3) |> kable_styling() |> scroll_box(height = "300px")
```

## CBCL internalising

```{r}
intcoef[order(intcoef$intRS, decreasing = T),] |> filter(abs(intRS)!=0) |> kbl(digits=3) |> kable_styling() |> scroll_box(height = "300px")
```

# Coefficients > 0.1 {.tabset}

```{r}
## filter coefficients to absolute value > 0.1
coef_01 = base_coefs_sig |> filter(abs(beta.min)>=0.1)
```

## Plot

```{r fig.width=10, fig.height=10}
ggplot(coef_01, aes(x = out, y = preds, fill = beta.min)) +
  geom_tile(color = "white", lwd = 1, linetype = 1) +
  geom_text(aes(label = round(beta.min, 3)), color = "black", size = 12, size.unit = "pt") +
  scale_fill_gradient2(name = "Standardised \n Beta", low = "#2166AC", high = "#B2182B") +
  xlab(NULL) + ylab(NULL) +
  theme(panel.grid = element_blank(), panel.background = element_blank(), 
        axis.text = element_text(size = 12), legend.title = element_text(size=12))

#ggsave(filename = "C:/Users/eilee/Desktop/PhD/Year 3/envRS/SCRIPTS/OUTPUT/baseline_01.svg", height = 140, width = 200, units = "mm")
```

## CBCL depression

```{r}
depcoef[order(depcoef$depRS, decreasing = T),] |> filter(abs(depRS)>=0.01) |> kbl(digits=3) |> kable_styling() |> scroll_box(height = "300px")
```

## CBCL internalising

```{r}
intcoef[order(intcoef$intRS, decreasing = T),] |> filter(abs(intRS)>=0.01) |> kbl(digits=3) |> kable_styling() |> scroll_box(height = "300px")
```


# Correlation plots {.tabset}

## Whole sample

```{r fig.width=10, fig.height=10}
rbind(train, test) |> select(-src_subject_id, -race_ethnicity) |> data.matrix() |> 
cor() |> 
corrplot::corrplot.mixed(sig.level = 0.05, upper = "color", lower = "number", order = "alphabet", tl.pos = "lt", tl.srt = 45, tl.cex=0.7, lower.col = "black", number.cex = 0.5, diag = "u") +
  theme(panel.grid = element_blank(), panel.background = element_blank())
```

## Training sample

```{r fig.width=10, fig.height=10}
## do the same plot here as for the whole sample correlation
train |> select(-src_subject_id, -race_ethnicity) |> 
  data.matrix() |> 
cor() |> 
corrplot::corrplot.mixed(sig.level = 0.05, upper = "color", lower = "number", order = "alphabet", tl.pos = "lt", tl.srt = 45, tl.cex=0.7, lower.col = "black", number.cex = 0.5, diag = "u") +
  theme(panel.grid = element_blank(), panel.background = element_blank())
```

## Test sample

```{r fig.width=10, fig.height=10}
## do the same plot here as for the whole sample correlation
test |> select(-src_subject_id, -race_ethnicity) |> 
  data.matrix() |> 
cor() |> 
corrplot::corrplot.mixed(sig.level = 0.05, upper = "color", lower = "number", order = "alphabet", tl.pos = "lt", tl.srt = 45, tl.cex=0.7, lower.col = "black", number.cex = 0.5, diag = "u") +
  theme(panel.grid = element_blank(), panel.background = element_blank())
```

# Descriptives {.tabset}

```{r}
#get age in years
sumdat = lapply(sumdat, function(x) {mutate(x, age_yrs = agemths/12)})
sumcols = names(sumdat$whole[,-1])
all_sum = get_sum_stats(df = sumdat$whole, vars = sumcols)
train_sum = get_sum_stats(df = sumdat$train, vars = sumcols)
test_sum = get_sum_stats(df = sumdat$test, vars = sumcols)
summaries = list(all_sum, train_sum, test_sum)
openxlsx::write.xlsx(summaries, "/exports/igmm/datastore/GenScotDepressionusers/eileen/ABCD/ABCD_Environmental_Risk/ABCDv5.1/OUTPUT/descriptives_cross.xlsx")
```


## Whole sample

```{r results='asis'}
summarytools::dfSummary(sumdat$whole[,-1],plain.ascii = F, style = "grid", valid.col = F, graph.col = T, varnumbers = F, tmp.img.dir = "/tmp", max.tbl.height = 300) |> print(method = "render")
```

## Training sample

```{r results='asis'}
summarytools::dfSummary(sumdat$train[,-1],plain.ascii = F, style = "grid", valid.col = F, graph.col = T, varnumbers = F, tmp.img.dir = "/tmp", max.tbl.height = 300) |> print(method = "render")
```

## Test sample

```{r results='asis'}
summarytools::dfSummary(sumdat$test[,-1],plain.ascii = F, style = "grid", valid.col = F, graph.col = T, varnumbers = F, tmp.img.dir = "/tmp", max.tbl.height = 300) |> print(method = "render")
```


 